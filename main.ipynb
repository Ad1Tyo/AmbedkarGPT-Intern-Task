{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb9fd86-bb04-4ad3-a522-69f4313632a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RAG System...\n",
      "First run - loading documents into vector store...\n",
      "Loading file: C:\\Users\\LENOVO\\Desktop\\Python\\speech.txt\n",
      "Created 1 document chunks\n",
      "Documents added successfully!\n",
      "\n",
      "============================================================\n",
      "RAG System Ready!\n",
      "============================================================\n",
      "Using model: mistral:latest\n",
      "Embedding model: all-minilm:l6-v2\n",
      "Vector store location: ./chroma_langchain_db\n",
      "============================================================\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask your question (or 'q' to quit):  who is real enemy?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching for relevant context...\n",
      "Found 1 relevant chunks. Generating answer...\n",
      "\n",
      "Answer:\n",
      "------------------------------------------------------------\n",
      " In the provided speech, the real enemy referred to is \"the belief in the sanctity of the shastras.\"\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Show source chunks? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source Chunks:\n",
      "============================================================\n",
      "\n",
      "[Chunk 1] (ID: 1)\n",
      "\"The real remedy is to destroy the belief in the sanctity of the shastras. How do you expect to succeed if you allow the shastras to continue to be held as sacred and infallible? You must take a stand against the scriptures. Either you must stop the practice of caste or you must stop believing in th...\n",
      "------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask your question (or 'q' to quit):  q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "\n",
    "\n",
    "FILE_PATH = r\"C:\\Users\\LENOVO\\Desktop\\Python\\speech.txt\"\n",
    "DB_LOCATION = \"./chroma_langchain_db\"\n",
    "COLLECTION_NAME = \"dbr_text\"\n",
    "MODEL_NAME = \"mistral:latest\"\n",
    "EMBEDDING_MODEL = \"all-minilm:l6-v2\"\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    \"\"\"Split text into overlapping chunks for better context\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        if chunk.strip():  \n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def load_documents(file_path):\n",
    "    \"\"\"Load and prepare documents from text file\"\"\"\n",
    "    print(f\"Loading file: {file_path}\")\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    \n",
    "    chunks = chunk_text(content, chunk_size=500, overlap=50)\n",
    "    \n",
    "    documents = []\n",
    "    ids = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, start=1):\n",
    "        document = Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"chunk_id\": i,\n",
    "                \"source\": os.path.basename(file_path)\n",
    "            },\n",
    "            id=str(i)\n",
    "        )\n",
    "        documents.append(document)\n",
    "        ids.append(str(i))\n",
    "    \n",
    "    print(f\"Created {len(documents)} document chunks\")\n",
    "    return documents, ids\n",
    "\n",
    "def initialize_vector_store(db_location, collection_name, embeddings):\n",
    "    \"\"\"Initialize or load existing vector store\"\"\"\n",
    "    add_documents = not os.path.exists(db_location)\n",
    "    \n",
    "    vector_store = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        persist_directory=db_location,\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    \n",
    "    return vector_store, add_documents\n",
    "\n",
    "def format_documents(docs):\n",
    "    \"\"\"Format retrieved documents for display\"\"\"\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        formatted.append(f\"[Chunk {i}]\\n{doc.page_content}\\n\")\n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "def main():\n",
    "    print(\"Initializing RAG System...\")\n",
    "    \n",
    "   \n",
    "    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
    "    \n",
    "   \n",
    "    vector_store, add_documents = initialize_vector_store(\n",
    "        DB_LOCATION, \n",
    "        COLLECTION_NAME, \n",
    "        embeddings\n",
    "    )\n",
    "    \n",
    "    \n",
    "    if add_documents:\n",
    "        print(\"First run - loading documents into vector store...\")\n",
    "        documents, ids = load_documents(FILE_PATH)\n",
    "        vector_store.add_documents(documents=documents, ids=ids)\n",
    "        print(\"Documents added successfully!\")\n",
    "    else:\n",
    "        print(\"Loading existing vector store...\")\n",
    "    \n",
    "   \n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5}\n",
    "    )\n",
    "    \n",
    "    \n",
    "    model = OllamaLLM(model=MODEL_NAME)\n",
    "    \n",
    "   \n",
    "    template = \"\"\"You are an expert at answering questions about a speech text.\n",
    "\n",
    "Use the following relevant excerpts from the speech to answer the question.\n",
    "If the answer cannot be found in the excerpts, say \"I cannot find that information in the provided speech.\"\n",
    "\n",
    "Relevant excerpts:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RAG System Ready!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Using model: {MODEL_NAME}\")\n",
    "    print(f\"Embedding model: {EMBEDDING_MODEL}\")\n",
    "    print(f\"Vector store location: {DB_LOCATION}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "   \n",
    "    while True:\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        question = input(\"Ask your question (or 'q' to quit): \").strip()\n",
    "        \n",
    "        if question.lower() == 'q':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if not question:\n",
    "            print(\"Please enter a question.\")\n",
    "            continue\n",
    "        \n",
    "        print(\"\\nSearching for relevant context...\")\n",
    "        \n",
    "        try:\n",
    "           \n",
    "            retrieved_docs = retriever.invoke(question)\n",
    "            \n",
    "            if not retrieved_docs:\n",
    "                print(\"No relevant context found.\")\n",
    "                continue\n",
    "            \n",
    "           \n",
    "            context = format_documents(retrieved_docs)\n",
    "            \n",
    "            print(f\"Found {len(retrieved_docs)} relevant chunks. Generating answer...\\n\")\n",
    "            \n",
    "           \n",
    "            result = chain.invoke({\n",
    "                \"context\": context,\n",
    "                \"question\": question\n",
    "            })\n",
    "            \n",
    "            print(\"Answer:\")\n",
    "            print(\"-\" * 60)\n",
    "            print(result)\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "           \n",
    "            show_sources = input(\"\\nShow source chunks? (y/n): \").strip().lower()\n",
    "            if show_sources == 'y':\n",
    "                print(\"\\nSource Chunks:\")\n",
    "                print(\"=\"*60)\n",
    "                for i, doc in enumerate(retrieved_docs, 1):\n",
    "                    print(f\"\\n[Chunk {i}] (ID: {doc.metadata.get('chunk_id', 'N/A')})\")\n",
    "                    print(doc.page_content[:300] + \"...\" if len(doc.page_content) > 300 else doc.page_content)\n",
    "                    print(\"-\"*60)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c16689-f713-44c1-8ef9-2e35b3d6c163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
